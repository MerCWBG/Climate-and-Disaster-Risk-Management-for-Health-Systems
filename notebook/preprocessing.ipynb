{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "base_directory = \"/Users/johnbarrera/Documents/Projects/world_bank/Climate-and-Disaster-Risk-Management-for-Health-Systems/\"\n",
    "sys.path.append(f\"{base_directory}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.config_reader import ConfigReader, Logger\n",
    "from src.utils.utils import GeoDataFrameOperations\n",
    "from src.utils.file_pocessor import FileLister, FileProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the Logger\n",
    "log_directory = \"/Users/johnbarrera/Documents/Projects/world_bank/Climate-and-Disaster-Risk-Management-for-Health-Systems/data/nepal/outputs/log\"\n",
    "log_file_name = \"preprocessing\"\n",
    "logger = Logger(log_directory, log_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Content of /Users/johnbarrera/Documents/Projects/world_bank/Climate-and-Disaster-Risk-Management-for-Health-Systems/config/nepal/setup_preprocessing.yaml file successfully read\n"
     ]
    }
   ],
   "source": [
    "# Readding the configuration file to the preprocessing\n",
    "config_file_path = \"config/nepal/setup_preprocessing.yaml\"\n",
    "config_file_path = f\"{base_directory}{config_file_path}\"\n",
    "\n",
    "try:\n",
    "    config_data = ConfigReader.read_yaml_file(config_file_path)\n",
    "    config_data = config_data['preprocessing']\n",
    "    txt_msg = \"Content of {} file successfully read\".format(config_file_path)\n",
    "    logger.info(txt_msg)\n",
    "except Exception as e:\n",
    "    txt_msg = f\"Error reading configuration file: {str(e)}\"\n",
    "    logger.error(txt_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: All elements are present in config_data list: ['hazards', 'infrastructures', 'population']\n",
      "INFO: Tasks successfully extracted from config data\n"
     ]
    }
   ],
   "source": [
    "# Identifying modules for preprocessing\n",
    "items_preprocessing = ['hazards', 'infrastructures', 'population']\n",
    "hazards_task = None\n",
    "infrastructures_task = None\n",
    "population_task = None\n",
    "\n",
    "items_config_data = [task for task in config_data]\n",
    "try:\n",
    "    for item in items_preprocessing:\n",
    "        if item not in items_config_data:\n",
    "            raise ValueError(f\"Item {item} not found in config_data list\")\n",
    "    txt_msg = f\"All elements are present in config_data list: {str(items_config_data)}\"\n",
    "    logger.info(txt_msg)\n",
    "except ValueError as e:\n",
    "    txt_msg = f\"'{e}'\"\n",
    "    logger.error(txt_msg)\n",
    "    \n",
    "\n",
    "try:\n",
    "    for task in items_config_data:\n",
    "        if task == \"hazards\":\n",
    "            hazards_task = config_data[task]\n",
    "        elif task == \"infrastructures\":\n",
    "            infrastructures_task = config_data[task]\n",
    "        elif task == \"population\":\n",
    "            population_task = config_data[task]\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected task '{task}' found in config data\")\n",
    "    txt_msg = f\"Tasks successfully extracted from config data\"\n",
    "    logger.info(txt_msg)\n",
    "except KeyError as e:\n",
    "    txt_msg = f\"Error: '{e}' not found in config data\"\n",
    "    logger.error(txt_msg)\n",
    "except ValueError as e:\n",
    "    txt_msg = f\"Error: {e}\"\n",
    "    logger.error(txt_msg)\n",
    "except Exception as e:\n",
    "    txt_msg = f\"An unexpected error occurred: {str(e)}\"\n",
    "    logger.error(txt_msg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write different types of messages to the log file\n",
    "# logger.info('Information message')\n",
    "# logger.warning('Warning message')\n",
    "# logger.error('Error message')\n",
    "# logger.critical('Critical message')\n",
    "# logger.debug('Debug message')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All periods files found\n",
      "Processing period 475: pga_2475.tif\n",
      "/Users/johnbarrera/Documents/Projects/world_bank/Climate-and-Disaster-Risk-Management-for-Health-Systems/data/nepal/inputs/hazards/Global_earthquake_hazard_WB/pga_2475.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 361/361 [00:01<00:00, 239.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing period 975: pga_975.tif\n",
      "/Users/johnbarrera/Documents/Projects/world_bank/Climate-and-Disaster-Risk-Management-for-Health-Systems/data/nepal/inputs/hazards/Global_earthquake_hazard_WB/pga_975.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 361/361 [00:01<00:00, 243.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing period 1500: pga_1500.tif\n",
      "/Users/johnbarrera/Documents/Projects/world_bank/Climate-and-Disaster-Risk-Management-for-Health-Systems/data/nepal/inputs/hazards/Global_earthquake_hazard_WB/pga_1500.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 361/361 [00:01<00:00, 226.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing period 2475: pga_2475.tif\n",
      "/Users/johnbarrera/Documents/Projects/world_bank/Climate-and-Disaster-Risk-Management-for-Health-Systems/data/nepal/inputs/hazards/Global_earthquake_hazard_WB/pga_2475.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 361/361 [00:01<00:00, 231.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing historical file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 361/361 [00:01<00:00, 230.91it/s]\n"
     ]
    }
   ],
   "source": [
    "# Processing Hazards task\n",
    "for task in hazards_task:\n",
    "    task_type = task['type']\n",
    "    task_historical = task['historical']\n",
    "    return_periods = task['return_periods']\n",
    "    substantial_damage = task['substantial_damage']\n",
    "    complete_destruction = task['complete_destruction']\n",
    "    input_path = f\"{base_directory}{task['source']}\"\n",
    "    output_path = f\"{base_directory}{task['destination']}\"\n",
    "\n",
    "    if task_historical:\n",
    "        txt_msg = f\"The file contents historical records for {task_type}\"\n",
    "        files = FileLister.list_files(input_path)\n",
    "        \n",
    "        try:\n",
    "            file = files[0]\n",
    "        except Exception as e:\n",
    "            txt_msg = f\"Error: '{e}'\"\n",
    "            logger.error(txt_msg)\n",
    "  \n",
    "            \n",
    "            \n",
    "        # Read valid files\n",
    "        file_name = file[0]\n",
    "        file_path = file[1]  \n",
    "            \n",
    "        txt_msg = f\"Processing historical file\"\n",
    "        print(txt_msg)\n",
    "        \n",
    "        # Transforming raster\n",
    "        gdf = FileProcessor.read_tif(file_path, 'polygon')\n",
    "        # Column damage generation\n",
    "        gdf = GeoDataFrameOperations.calculate_damage(gdf, substantial_damage, complete_destruction)\n",
    "        # Save file\n",
    "        output_name = f'{task_type}_historical.gpkg'\n",
    "        FileProcessor.save_to_geopackage(gdf, output_path, output_name)\n",
    "\n",
    "        \n",
    "    else:\n",
    "        txt_msg = f\"The file contents periodical records for {task_type}\"\n",
    "        files = FileLister.list_files(input_path)\n",
    "        valid_files = []\n",
    "        counters = 1\n",
    "        for period in return_periods:\n",
    "            validate_period = False\n",
    "            counters = 0\n",
    "            while not validate_period and counters <len(files):\n",
    "                if str(period) in files[counters][0]:\n",
    "                    validate_period = True\n",
    "                    valid_files.append([period, files[counters][0], files[counters][1]])\n",
    "                counters+=1\n",
    "        \n",
    "        if set(return_periods) != set([f[0] for f in valid_files]):\n",
    "            elements_not_in_set = set(return_periods)  - set([f[0] for f in valid_files])\n",
    "            txt_msg = f\"Some periods files not found {elements_not_in_set}\"\n",
    "            print(txt_msg)\n",
    "        else:\n",
    "            txt_msg = f\"All periods files found\"\n",
    "            print(txt_msg)\n",
    "            \n",
    "                    \n",
    "        # Read valid files \n",
    "        for file in valid_files:\n",
    "            file_period = file[0]\n",
    "            file_name = file[1]\n",
    "            file_path = file[2]\n",
    "            \n",
    "            txt_msg = f\"Processing period {file_period}: {file_name}\"\n",
    "            print(txt_msg)\n",
    "            \n",
    "            txt_msg = f\"{file_path}\"\n",
    "            print(txt_msg)\n",
    "            \n",
    "            # Transforming raster\n",
    "            gdf = FileProcessor.read_tif(file_path, 'polygon')\n",
    "            # Column damage generation\n",
    "            gdf = GeoDataFrameOperations.calculate_damage(gdf, substantial_damage, complete_destruction)\n",
    "            # print(gdf['value'].sum())\n",
    "            # Save file\n",
    "            output_name = f'{task_type}_period_{file_period}.gpkg'\n",
    "            FileProcessor.save_to_geopackage(gdf, output_path, output_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing population file: /Users/johnbarrera/Documents/Projects/world_bank/Climate-and-Disaster-Risk-Management-for-Health-Systems/data/nepal/inputs/popu/nepal_npl_ct_popu_pop_sp_py_GHS_2023_p_u_Clipped_E2020_Nepal_4326.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4919/4919 [01:52<00:00, 43.77it/s]\n",
      "/var/folders/yh/mnw4_865319d05f6fkrpt6280000gn/T/ipykernel_38425/1101443751.py:13: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  gdf['geometry'] = gdf.geometry.centroid\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing population file: /Users/johnbarrera/Documents/Projects/world_bank/Climate-and-Disaster-Risk-Management-for-Health-Systems/data/nepal/inputs/popu/nepal_npl_ct_popu_pop_sp_py_GHS_2023_p_u_Clipped_E2020_Nepal_54009.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5511/5511 [01:52<00:00, 48.84it/s]\n",
      "/var/folders/yh/mnw4_865319d05f6fkrpt6280000gn/T/ipykernel_38425/1101443751.py:13: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  gdf['geometry'] = gdf.geometry.centroid\n"
     ]
    }
   ],
   "source": [
    "# Processing Population task\n",
    "for task in population_task:\n",
    "    task_type = task['type']\n",
    "    input_path = f\"{base_directory}{task['source']}\"\n",
    "    output_path = f\"{base_directory}{task['destination']}\"\n",
    "    \n",
    "    txt_msg = f\"Processing population file: {input_path}\"\n",
    "    print(txt_msg)\n",
    "    # Transforming raster\n",
    "    gdf = FileProcessor.read_tif(input_path, 'polygon')\n",
    "    gdf.columns = ['band', 'population', 'geometry']\n",
    "    gdf['geometry'] = gdf.geometry.centroid\n",
    "    gdf = gdf[['population', 'geometry']]\n",
    "    \n",
    "    \n",
    "    output_name = f'{task_type}.gpkg'\n",
    "    FileProcessor.save_to_geopackage(gdf, output_path, output_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'type': 'PHC', 'filter': [\"HF_T_RO=='Primary Health Care Center'\"], 'source': 'data/nepal/inputs/heal/nepal_npl_ct_heal_heal_sp_tab_NDRRNA_14022024.gpkg', 'destination': 'data/nepal/outputs/infrastructure/'}\n",
      "Processing infrastructure file: PHC\n",
      "{'type': 'healt_facilities', 'filter': [\"HF_T_RO!='Primary Health Care Center'\"], 'source': 'data/nepal/inputs/heal/nepal_npl_ct_heal_heal_sp_tab_NDRRNA_14022024.gpkg', 'destination': 'data/nepal/outputs/infrastructure/'}\n",
      "Processing infrastructure file: healt_facilities\n"
     ]
    }
   ],
   "source": [
    "# Processing Infrastructure task\n",
    "for task in infrastructures_task:\n",
    "    print(task)\n",
    "    task_type = task['type']\n",
    "    task_filter= task['filter']\n",
    "    input_path = f\"{base_directory}{task['source']}\"\n",
    "    output_path = f\"{base_directory}{task['destination']}\"\n",
    "    \n",
    "    gdf = FileProcessor.read_geopackage(input_path)\n",
    "    if task_filter is not None:\n",
    "        filter_str = task_filter[0]\n",
    "        gdf = gdf.query(filter_str)\n",
    "        gdf = gdf.reset_index(drop=True)\n",
    "        \n",
    "    txt_msg = f\"Processing infrastructure file: {task_type}\"\n",
    "    print(txt_msg)\n",
    "    output_name = f'{task_type}_infrastructure.gpkg'\n",
    "    FileProcessor.save_to_geopackage(gdf, output_path, output_name)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3_enviroment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
